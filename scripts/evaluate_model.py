import torch
import networkx as nx
from ai_engine.model import load_model
from ai_engine.generator import PCBGenerator
from ai_engine.constraint_cost import calculate_constraint_cost, validate_constraints
from ai_engine.graph_utils import (
    get_graph_metrics,
    normalize_graph,
    enrich_graph_features,
)
from security.analyzer import SecurityAnalyzer
import numpy as np
from typing import Dict, List
import json
import time


class ModelEvaluator:
    def __init__(self, model_path="pcb_gnn.pt"):
        self.model = load_model(model_path) if self._model_exists(model_path) else None
        self.generator = PCBGenerator(model_path)
        self.security_analyzer = SecurityAnalyzer()

    def _model_exists(self, path):
        import os

        return os.path.exists(path)

    def evaluate_generation_quality(self, test_specs: List[Dict]) -> Dict:
        """
        Evaluate the quality of PCB layouts generated by the model
        """
        results = {
            "average_trace_length": 0,
            "routing_success_rate": 0,
            "constraint_violations": 0,
            "security_score": 0,
            "generation_time": 0,
            "connectivity_rate": 0,
            "efficiency_score": 0,
            "accuracy_improvement": 0,
        }

        total_trace_length = 0
        successful_generations = 0
        total_constraint_violations = 0
        total_security_score = 0
        total_connectivity = 0
        total_efficiency = 0
        generation_times = []

        for spec in test_specs:
            try:
                start_time = time.time()

                pcb_graph, metrics = self.generator.generate_layout(spec)

                end_time = time.time()
                generation_times.append(end_time - start_time)

                # Calculate metrics
                total_trace_length += metrics["avg_trace_length"]

                # Check routing success (non-zero edges)
                if len(pcb_graph["edges"]) > 0:
                    successful_generations += 1

                # Count constraint violations
                graph_for_validation = self._dict_to_networkx(pcb_graph)
                _, costs = calculate_constraint_cost(graph_for_validation, spec)
                total_constraint_violations += sum(costs.values())

                # Security analysis
                vulnerabilities, sec_score = self.security_analyzer.analyze(pcb_graph)
                total_security_score += sec_score

                # Connectivity
                if len(pcb_graph["nodes"]) > 1:
                    graph = self._dict_to_networkx(pcb_graph)
                    if nx.is_connected(graph):
                        total_connectivity += 1

                # Efficiency score
                total_efficiency += metrics.get("efficiency_score", 0)

            except Exception as e:
                print(f"Error evaluating spec: {str(e)}")
                continue

        n_evaluated = len(test_specs)
        if n_evaluated > 0:
            results["average_trace_length"] = total_trace_length / n_evaluated
            results["routing_success_rate"] = successful_generations / n_evaluated
            results["constraint_violations"] = total_constraint_violations / n_evaluated
            results["security_score"] = total_security_score / n_evaluated
            results["generation_time"] = (
                sum(generation_times) / len(generation_times) if generation_times else 0
            )
            results["connectivity_rate"] = total_connectivity / n_evaluated
            results["efficiency_score"] = total_efficiency / n_evaluated
            # Accuracy improvement based on multiple factors
            results["accuracy_improvement"] = (
                results["security_score"] * 0.3
                + (1 - min(results["constraint_violations"] / 10, 1)) * 0.3
                + results["connectivity_rate"] * 0.2
                + results["routing_success_rate"] * 0.2
            )

        return results

    def evaluate_accuracy_vs_baseline(self, test_specs: List[Dict]) -> Dict:
        """
        Compare accuracy against a simple baseline (random or grid-based)
        """
        baseline_results = self._run_baseline_evaluation(test_specs)
        model_results = self.evaluate_generation_quality(test_specs)

        improvement_metrics = {}
        for key in model_results:
            if (
                isinstance(model_results[key], (int, float))
                and key != "generation_time"
            ):
                baseline_val = baseline_results.get(key, 0)
                model_val = model_results[key]

                if baseline_val != 0:
                    improvement = (model_val - baseline_val) / abs(baseline_val) * 100
                else:
                    improvement = (
                        model_val * 100 if model_val >= 0 else -abs(model_val) * 100
                    )

                improvement_metrics[f"{key}_improvement_%"] = improvement

        return {
            "baseline_results": baseline_results,
            "model_results": model_results,
            "improvements": improvement_metrics,
        }

    def _run_baseline_evaluation(self, test_specs: List[Dict]) -> Dict:
        """
        Run evaluation with a simple baseline approach
        """
        # Baseline: simple grid layout
        results = {
            "average_trace_length": 0,
            "routing_success_rate": 0,
            "constraint_violations": 0,
            "security_score": 0,
            "generation_time": 0,
            "connectivity_rate": 0,
            "efficiency_score": 0,
        }

        total_trace_length = 0
        successful_generations = 0
        total_constraint_violations = 0
        total_security_score = 0
        total_connectivity = 0
        total_efficiency = 0

        for spec in test_specs:
            # Create simple grid layout
            n_components = spec.get("component_count", 8)
            graph = nx.Graph()

            # Place components in grid
            grid_size = int(np.ceil(np.sqrt(n_components)))
            for i in range(n_components):
                row = i // grid_size
                col = i % grid_size
                graph.add_node(i, position=[col * 5, row * 5], signal_type="signal")

            # Connect in sequence
            for i in range(n_components - 1):
                pos1 = graph.nodes[i]["position"]
                pos2 = graph.nodes[i + 1]["position"]
                dist = np.sqrt((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2)
                graph.add_edge(i, i + 1, length=dist, width=0.2, layer=0)

            # Convert to dict format
            pcb_graph = self._nx_to_dict(graph)

            # Calculate metrics similar to model
            if len(graph.edges()) > 0:
                successful_generations += 1
                total_trace_length += sum(
                    d["length"] for u, v, d in graph.edges(data=True)
                ) / len(graph.edges())

            _, costs = calculate_constraint_cost(graph, spec)
            total_constraint_violations += sum(costs.values())

            vulnerabilities, sec_score = self.security_analyzer.analyze(pcb_graph)
            total_security_score += sec_score

            if nx.is_connected(graph) if len(graph.nodes()) > 1 else True:
                total_connectivity += 1

            total_efficiency += (
                len(graph.edges()) / len(graph.nodes()) if len(graph.nodes()) > 0 else 0
            )

        n_evaluated = len(test_specs)
        if n_evaluated > 0:
            results["average_trace_length"] = total_trace_length / n_evaluated
            results["routing_success_rate"] = successful_generations / n_evaluated
            results["constraint_violations"] = total_constraint_violations / n_evaluated
            results["security_score"] = total_security_score / n_evaluated
            results["connectivity_rate"] = total_connectivity / n_evaluated
            results["efficiency_score"] = total_efficiency / n_evaluated

        return results

    def evaluate_diversity(self, spec: Dict, num_samples: int = 10) -> Dict:
        """
        Evaluate the diversity of generated layouts for the same specification
        """
        layouts = []
        for i in range(num_samples):
            pcb_graph, _ = self.generator.generate_layout(spec)
            layouts.append(pcb_graph)

        # Calculate diversity metrics
        avg_pairwise_distance = self._calculate_layout_diversity(layouts)

        return {"diversity_score": avg_pairwise_distance, "num_samples": num_samples}

    def _calculate_layout_diversity(self, layouts: List[Dict]) -> float:
        """
        Calculate diversity as average pairwise distance between layouts
        """
        if len(layouts) < 2:
            return 0.0

        distances = []
        for i in range(len(layouts)):
            for j in range(i + 1, len(layouts)):
                dist = self._layout_distance(layouts[i], layouts[j])
                distances.append(dist)

        return sum(distances) / len(distances) if distances else 0.0

    def _layout_distance(self, layout1: Dict, layout2: Dict) -> float:
        """
        Calculate distance between two layouts (simplified)
        """
        # Compare number of nodes and edges
        node_diff = abs(len(layout1["nodes"]) - len(layout2["nodes"]))
        edge_diff = abs(len(layout1["edges"]) - len(layout2["edges"]))

        # Compare average positions
        if layout1["nodes"] and layout2["nodes"]:
            avg_pos1 = np.mean(
                [
                    [n["position"][0], n["position"][1]]
                    for n in layout1["nodes"].values()
                ],
                axis=0,
            )
            avg_pos2 = np.mean(
                [
                    [n["position"][0], n["position"][1]]
                    for n in layout2["nodes"].values()
                ],
                axis=0,
            )
            pos_diff = np.linalg.norm(avg_pos1 - avg_pos2)
        else:
            pos_diff = 0

        return node_diff + edge_diff + pos_diff

    def _dict_to_networkx(self, pcb_graph: Dict) -> nx.Graph:
        """Convert PCB graph dict to NetworkX graph"""
        graph = nx.Graph()

        for node_id, node_data in pcb_graph["nodes"].items():
            graph.add_node(node_id, **node_data)

        for edge_id, edge_data in pcb_graph["edges"].items():
            source = edge_data["source"]
            target = edge_data["target"]
            graph.add_edge(source, target, **edge_data)

        return graph

    def _nx_to_dict(self, graph: nx.Graph) -> Dict:
        """Convert NetworkX graph to dict format"""
        result = {"nodes": {}, "edges": {}}

        for node_id, data in graph.nodes(data=True):
            result["nodes"][str(node_id)] = data

        for src, tgt, data in graph.edges(data=True):
            edge_key = f"{src}-{tgt}"
            result["edges"][edge_key] = {"source": str(src), "target": str(tgt), **data}

        return result

    def run_comprehensive_evaluation(self) -> Dict:
        """
        Run comprehensive evaluation of the model
        """
        # Define test specifications
        test_specs = [
            {
                "component_count": 5,
                "max_trace_length": 20.0,
                "layers": 2,
                "power_domains": ["3.3V"],
                "signal_types": ["digital"],
                "constraints": {},
                "area": 50,
            },
            {
                "component_count": 8,
                "max_trace_length": 30.0,
                "layers": 4,
                "power_domains": ["3.3V", "5V"],
                "signal_types": ["digital", "analog"],
                "constraints": {},
                "area": 100,
            },
            {
                "component_count": 12,
                "max_trace_length": 50.0,
                "layers": 2,
                "power_domains": ["1.8V", "3.3V"],
                "signal_types": ["digital", "clock", "reset"],
                "constraints": {},
                "area": 150,
            },
            {
                "component_count": 15,
                "max_trace_length": 40.0,
                "layers": 6,
                "power_domains": ["1.2V", "1.8V", "3.3V"],
                "signal_types": ["digital", "analog", "high_speed"],
                "constraints": {},
                "area": 200,
            },
        ]

        print("Evaluating model quality...")
        quality_results = self.evaluate_generation_quality(test_specs)

        print("Comparing with baseline...")
        comparison_results = self.evaluate_accuracy_vs_baseline(
            test_specs[:2]
        )  # Use subset for baseline comparison

        print("Evaluating diversity...")
        diversity_results = self.evaluate_diversity(test_specs[0])

        comprehensive_results = {
            "quality_metrics": quality_results,
            "comparison_with_baseline": comparison_results,
            "diversity_metrics": diversity_results,
            "overall_score": self._calculate_overall_score(quality_results),
        }

        return comprehensive_results

    def _calculate_overall_score(self, quality_results: Dict) -> float:
        """
        Calculate overall model performance score
        """
        # Weighted combination of different metrics
        weights = {
            "routing_success_rate": 0.25,
            "security_score": 0.2,
            "connectivity_rate": 0.15,
            "efficiency_score": 0.15,
            "accuracy_improvement": 0.15,  # New metric for GNN accuracy
            "constraint_violations": -0.1,  # Negative weight since lower is better
        }

        score = 0
        for metric, weight in weights.items():
            value = quality_results.get(metric, 0)
            # Normalize constraint violations (invert so fewer violations = higher score)
            if metric == "constraint_violations":
                value = max(0, min(1, 1 / (1 + value / 10)))  # Normalize and invert
            elif metric == "generation_time":
                # Invert time (faster is better)
                value = max(0, min(1, 1 / (1 + value))) if value > 0 else 1

            score += weight * value

        return max(0, min(1, score))  # Clamp between 0 and 1


def print_evaluation_report(results: Dict):
    """
    Print a formatted evaluation report
    """
    print("=" * 70)
    print("PUNREK ADVANCED GNN MODEL EVALUATION REPORT")
    print("=" * 70)

    print("\nQUALITY METRICS:")
    qm = results["quality_metrics"]
    print(f"  Average Trace Length: {qm['average_trace_length']:.2f}")
    print(f"  Routing Success Rate: {qm['routing_success_rate']:.2f}")
    print(f"  Constraint Violations: {qm['constraint_violations']:.2f}")
    print(f"  Security Score: {qm['security_score']:.2f}")
    print(f"  Generation Time: {qm['generation_time']:.3f}s")
    print(f"  Connectivity Rate: {qm['connectivity_rate']:.2f}")
    print(f"  Efficiency Score: {qm['efficiency_score']:.2f}")
    print(f"  Accuracy Improvement: {qm['accuracy_improvement']:.2f}")

    print("\nBASELINE COMPARISON:")
    comp = results["comparison_with_baseline"]
    print("  Model vs Baseline Improvements:")
    for metric, improvement in comp["improvements"].items():
        print(f"    {metric}: {improvement:+.1f}%")

    print("\nDIVERSITY METRICS:")
    dm = results["diversity_metrics"]
    print(f"  Diversity Score: {dm['diversity_score']:.2f}")
    print(f"  Samples Generated: {dm['num_samples']}")

    print("\nOVERALL PERFORMANCE:")
    print(f"  Overall Score: {results['overall_score']:.3f}")

    # Performance classification
    score = results["overall_score"]
    if score >= 0.85:
        perf_level = "EXCELLENT - Highly Accurate GNN Model"
    elif score >= 0.7:
        perf_level = "VERY GOOD - Accurate GNN Model"
    elif score >= 0.55:
        perf_level = "GOOD - Moderately Accurate"
    elif score >= 0.4:
        perf_level = "FAIR - Basic Functionality"
    else:
        perf_level = "POOR - Needs Improvement"

    print(f"  Performance Level: {perf_level}")
    print("=" * 70)


if __name__ == "__main__":
    evaluator = ModelEvaluator()

    print("Starting comprehensive model evaluation...")
    results = evaluator.run_comprehensive_evaluation()

    print_evaluation_report(results)

    # Save results to file
    with open("evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)

    print("\nEvaluation results saved to 'evaluation_results.json'")
